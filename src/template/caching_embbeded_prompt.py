import os
import json
import redis
import hashlib
from dotenv import load_dotenv
from typing import List, Dict

from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_community.vectorstores import FAISS
from langchain.retrievers.document_compressors import EmbeddingsFilter
from langchain.retrievers import ContextualCompressionRetriever

# âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë”©
def get_env_variable(key: str) -> str:
    load_dotenv()
    value = os.getenv(key)
    if not value:
        raise ValueError(f"Missing environment variable: {key}")
    return value

# âœ… Redis ì—°ê²°
def create_redis_client():
    return redis.Redis(
        host=get_env_variable("REDIS_HOST"),
        port=int(get_env_variable("REDIS_PORT")),
        db=int(get_env_variable("REDIS_DB")),
        decode_responses=False
    )

# âœ… ì„ë² ë”© ìºì‹±
def get_or_cache_embedding(query: str, embeddings: OpenAIEmbeddings, redis_client) -> List[float]:
    query_hash = hashlib.sha256(query.encode()).hexdigest()
    cached = redis_client.get(query_hash)
    if cached:
        return json.loads(cached.decode())

    vector = embeddings.embed_query(query)
    redis_client.setex(query_hash, 86400, json.dumps(vector))  # TTL 1ì¼
    return vector

# âœ… LLM ìƒì„±
def create_llm(api_key: str) -> ChatOpenAI:
    return ChatOpenAI(temperature=0.3, openai_api_key=api_key)

# âœ… í”„ë¡¬í”„íŠ¸ ë¡œë”©
def load_prompt_from_file(path: str) -> PromptTemplate:
    with open(path, "r", encoding="utf-8") as file:
        template_str = file.read()
    return PromptTemplate.from_template(template_str)

# âœ… í‚¤ì›Œë“œ ì¶”ì¶œ ì²´ì¸ ìƒì„±
def extract_keywords_chain(llm: ChatOpenAI):
    keyword_prompt = PromptTemplate.from_template(
        """
        ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œì™€ ì¹´í…Œê³ ë¦¬ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
        ì§ˆë¬¸: {query}

        ì¶œë ¥ í˜•ì‹:
        {{
            "keywords": ["..."],
            "category": "..."
        }}
        """
    )
    return keyword_prompt | llm | RunnableLambda(lambda x: json.loads(x.content))

# âœ… ë¬¸ì„œ ê²€ìƒ‰ ì²´ì¸ ìƒì„±
def create_conversation_chain(llm: ChatOpenAI, documents: List[Dict[str, str]], api_key: str):
    embeddings = OpenAIEmbeddings(openai_api_key=api_key)
    text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)
    split_docs = text_splitter.create_documents([doc["text"] for doc in documents])

    vector_store = FAISS.from_documents(split_docs, embeddings)
    retriever = vector_store.as_retriever()

    compression_retriever = ContextualCompressionRetriever(
        base_compressor=EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76),
        base_retriever=retriever
    )

    try:
        prompt = load_prompt_from_file("../resources/prompt/embedding/embedding_prompt.txt")
    except FileNotFoundError:
        prompt = PromptTemplate.from_template(
            "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n\n{context}\n\nì§ˆë¬¸: {input}\në‹µë³€:"
        )

    document_chain = create_stuff_documents_chain(llm, prompt)
    return compression_retriever, document_chain, vector_store

# âœ… ë©”ì¸

def main():
    try:
        api_key = get_env_variable("CHAT_GPT_API_KEY")
        llm = create_llm(api_key)
        redis_client = create_redis_client()
        embeddings = OpenAIEmbeddings(openai_api_key=api_key)

        documents = [
            {"text": "ì„œë²„ì—ì„œ 502 ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤."},
            {"text": "ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."},
            {"text": "ì‹œìŠ¤í…œì€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤."}
        ]

        keyword_extractor = extract_keywords_chain(llm)
        compression_retriever, document_chain, vector_store = create_conversation_chain(llm, documents, api_key)

        queries = [
            "ê²Œì´íŠ¸ì›¨ì´ ì˜¤ë¥˜ê°€ ìˆì—ˆì–´.",
            "ì„œë²„ê°€ ì‘ë™ ì¤‘ì§€ ëë‹¤ê³ ?",
            "ë””ìŠ¤í¬ ë¬¸ì œëŠ” ì™œ ë°œìƒí–ˆì§€?"
        ]

        for query in queries:
            print(f"\nğŸ“ ì§ˆë¬¸: {query}")
            try:
                keyword_result = keyword_extractor.invoke({"query": query})
                print(f"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼: {keyword_result}")

                query_vector = get_or_cache_embedding(query, embeddings, redis_client)
                docs = vector_store.similarity_search_by_vector(query_vector, k=3)
                response = document_chain.invoke({"context": docs, "input": query})

                # ì•ˆì „í•˜ê²Œ ì¶œë ¥
                if isinstance(response, dict) and "answer" in response:
                    print(f"ğŸ“˜ ë‹µë³€: {response['answer']}")
                else:
                    print(f"ğŸ“˜ ë‹µë³€: {response}")

            except Exception as e:
                print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    except Exception as e:
        print(f"âŒ ì „ì²´ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
