import os
import json
from dotenv import load_dotenv
from typing import List, Dict

from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_core.messages import AIMessage
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.retrievers import ContextualCompressionRetriever
from langchain_community.vectorstores import FAISS
from langchain.retrievers.document_compressors import EmbeddingsFilter


def get_env_variable(key: str) -> str:
    load_dotenv()
    value = os.getenv(key)
    if not value:
        raise ValueError(f"Missing environment variable: {key}")
    return value


def create_llm(api_key: str) -> ChatOpenAI:
    return ChatOpenAI(temperature=0.3, openai_api_key=api_key)


def load_prompt_from_file(path: str) -> PromptTemplate:
    with open(path, "r", encoding="utf-8") as file:
        template_str = file.read()
    return PromptTemplate.from_template(template_str)


def extract_keywords_chain(llm: ChatOpenAI):
    keyword_prompt = PromptTemplate.from_template(
        """
        ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œì™€ ì¹´í…Œê³ ë¦¬ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”.
        ì§ˆë¬¸: {query}

        ì¶œë ¥ í˜•ì‹:
        {{
            "keywords": ["..."],
            "category": "..."
        }}
        """
    )

    return keyword_prompt | llm | RunnableLambda(lambda x: json.loads(x.content))


def create_conversation_chain(llm: ChatOpenAI, documents: List[Dict[str, str]], api_key: str):
    embeddings = OpenAIEmbeddings(openai_api_key=api_key)
    text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)
    split_docs = text_splitter.create_documents([doc["text"] for doc in documents])

    vector_store = FAISS.from_documents(split_docs, embeddings)
    retriever = vector_store.as_retriever()

    compression_retriever = ContextualCompressionRetriever(
        base_compressor=EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76),
        base_retriever=retriever
    )

    try:
        prompt = load_prompt_from_file("../resources/prompt/embedding/embedding_prompt.txt")
    except FileNotFoundError:
        prompt = PromptTemplate.from_template(
            "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n\n{context}\n\nì§ˆë¬¸: {input}\në‹µë³€:"
        )

    document_chain = create_stuff_documents_chain(llm, prompt)
    return create_retrieval_chain(compression_retriever, document_chain)


def main():
    try:
        api_key = get_env_variable("CHAT_GPT_API_KEY")
        llm = create_llm(api_key)

        documents = [
            {"text": "ì„œë²„ì—ì„œ 502 ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤."},
            {"text": "ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."},
            {"text": "ì‹œìŠ¤í…œì€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤."}
        ]

        keyword_extractor = extract_keywords_chain(llm)
        conversation_chain = create_conversation_chain(llm, documents, api_key)

        queries = [
            "ê²Œì´íŠ¸ì›¨ì´ ì˜¤ë¥˜ê°€ ìˆì—ˆì–´.",
            "ì„œë²„ê°€ ì‘ë™ ì¤‘ì§€ ëë‹¤ê³ ?",
            "ë””ìŠ¤í¬ ë¬¸ì œëŠ” ì™œ ë°œìƒí–ˆì§€?"
        ]

        for query in queries:
            try:
                print(f"\nğŸ“ ì§ˆë¬¸: {query}")
                keyword_response = keyword_extractor.invoke({"query": query})

                print(f"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼: {keyword_response}")

                response = conversation_chain.invoke({"input": query})
                print(f"ğŸ“˜ ë‹µë³€: {response['answer']}")
            except Exception as e:
                print(f"âŒ í‚¤ì›Œë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    except Exception as e:
        print(f"âŒ ì „ì²´ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()
