import os
from dotenv import load_dotenv
from typing import List, Dict

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain.retrievers import ContextualCompressionRetriever
from langchain_community.vectorstores import FAISS
from langchain.retrievers.document_compressors import EmbeddingsFilter


# âœ… í™˜ê²½ë³€ìˆ˜ ë¡œë”©
def get_env_variable(key: str) -> str:
    load_dotenv()
    value = os.getenv(key)
    if not value:
        raise ValueError(f"Missing environment variable: {key}")
    return value


# âœ… LLM êµ¬ì„±
def create_llm(api_key: str) -> ChatOpenAI:
    return ChatOpenAI(temperature=0.3, openai_api_key=api_key)


# âœ… í”„ë¡¬í”„íŠ¸ ë¡œë”©
def load_prompt_from_file(path: str) -> PromptTemplate:
    with open(path, "r", encoding="utf-8") as file:
        template_str = file.read()
    return PromptTemplate.from_template(template_str)


# âœ… ì „ì²´ ì²´ì¸ êµ¬ì„±
def create_conversation_chain(llm: ChatOpenAI, documents: List[Dict[str, str]], api_key: str):
    embeddings = OpenAIEmbeddings(openai_api_key=api_key)
    text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)
    split_docs = text_splitter.create_documents([doc["text"] for doc in documents])

    # âœ… FAISS ë²¡í„° ì €ì¥ì†Œ ì‚¬ìš©
    vector_store = FAISS.from_documents(split_docs, embeddings)

    retriever = vector_store.as_retriever()
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.76),
        base_retriever=retriever
    )

    try:
        prompt = load_prompt_from_file("../resources/prompt/embedding/embedding_prompt.txt")
    except FileNotFoundError:
        prompt = PromptTemplate.from_template(
            "ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n\n{context}\n\nì§ˆë¬¸: {input}\në‹µë³€:"
        )

    document_chain = create_stuff_documents_chain(llm, prompt)
    return create_retrieval_chain(compression_retriever, document_chain)


# âœ… ì‹¤í–‰
def main():
    try:
        api_key = get_env_variable("CHAT_GPT_API_KEY")
        llm = create_llm(api_key)

        documents = [
            {"text": "ì„œë²„ì—ì„œ 502 ì—ëŸ¬ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤."},
            {"text": "ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±ìœ¼ë¡œ ì¸í•´ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."},
            {"text": "ì‹œìŠ¤í…œì€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤."}
        ]

        chain = create_conversation_chain(llm, documents, api_key)

        queries = [
            "ê²Œì´íŠ¸ì›¨ì´ ì˜¤ë¥˜ê°€ ìˆì—ˆì–´.",
            "ì„œë²„ê°€ ì‘ë™ ì¤‘ì§€ ëë‹¤ê³ ?",
            "ë””ìŠ¤í¬ ë¬¸ì œëŠ” ì™œ ë°œìƒí–ˆì§€?"
        ]

        for query in queries:
            response = chain.invoke({"input": query})
            print(f"\nğŸ“ ì§ˆë¬¸: {query}")
            print(f"ğŸ“˜ ë‹µë³€: {response['answer']}")

    except Exception as e:
        print(f"âŒ An error occurred: {e}")


if __name__ == "__main__":
    main()
