# ğŸ”— LangChain ì²´ì´ë‹(Chaining)ê³¼ ë¸Œëœì¹­(Branching) ì •ë¦¬

## âœ… ì²´ì´ë‹ (Chaining)

### ğŸ§  ì •ì˜
- ì—¬ëŸ¬ ê°œì˜ ì²˜ë¦¬ ë‹¨ê³„ë¥¼ **ì„ í˜•(ì§ë ¬)** ìœ¼ë¡œ ì—°ê²°í•˜ì—¬, ì• ë‹¨ê³„ì˜ ì¶œë ¥ì´ ë‹¤ìŒ ë‹¨ê³„ì˜ ì…ë ¥ì´ ë˜ë„ë¡ êµ¬ì„±í•˜ëŠ” ë°©ì‹
- `RunnableSequence` ë¥¼ í†µí•´ êµ¬í˜„ë¨

### ğŸ§± êµ¬ì¡° ì˜ˆì‹œ

```text
[PromptTemplate1] â†’ [LLM1] â†’ [PromptTemplate2] â†’ [LLM2] â†’ ê²°ê³¼
```

ë˜ëŠ”

```python
from langchain_core.runnables import RunnableSequence

chain = prompt1 | llm1 | prompt2 | llm2
response = chain.invoke({"input": "ì—ëŸ¬ ë¡œê·¸ ë°œìƒ"})
```

## ğŸ§° ì‚¬ìš© ì˜ˆ

1. ì—ëŸ¬ ë¡œê·¸ â†’ ìƒíƒœ íŒë‹¨ (JSON ë°˜í™˜)
2. JSONì—ì„œ ì—ëŸ¬ ì½”ë“œ ì¶”ì¶œ â†’ ì„¤ëª… í”„ë¡¬í”„íŠ¸ë¡œ ì „ë‹¬
3. ìµœì¢… ì‘ë‹µ ìƒì„±

---

## ğŸŒ¿ ë¸Œëœì¹­ (Branching)

### ğŸŒŸ ì •ì˜
- ì¡°ê±´ì— ë”°ë¼ ë‹¤ë¥¸ ì²´ì¸ ë˜ëŠ” ë¡œì§ì„ ë¶„ê¸°ì‹œì¼œ ì‹¤í–‰í•˜ëŠ” ë°©ì‹
- ì²´ì´ë‹ ê²°ê³¼ì˜ ìƒíƒœë‚˜ ì†ì„±ì— ë”°ë¼ ê²½ë¡œë¥¼ ë‹¬ë¦¬í•¨ (ì˜ˆ: "status": "ok" â†’ ì„¤ëª… ì²´ì¸, "ignored" â†’ ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•ŠìŒ)

### ğŸ”§ êµ¬í˜„ ë°©ì‹ (Python)

```python
if status == "ok":
    response = ok_chain.invoke({...})
elif status == "generated":
    response = generated_chain.invoke({...})
else:
    response = ignored_chain.invoke({...})
```

ë˜ëŠ” RunnableLambdaë¥¼ í†µí•œ ë™ì  ë¼ìš°íŒ… ë°©ì‹ë„ ê°€ëŠ¥

---

## ğŸ”„ ì²´ì´ë‹ + ë¸Œëœì¹­ í†µí•© êµ¬ì¡° ì˜ˆì‹œ

```text
[Input Log]
   â†“
[PromptTemplate: JSON íŒë‹¨] â”€â”€â†’ {status: "ok", error_code: "502"}
   â†“
[Branching ì²˜ë¦¬]
   â”œâ”€ "ok"         â†’ [PromptTemplate: ì½”ë“œ ì„¤ëª…] â†’ [LLM]
   â”œâ”€ "generated"  â†’ [PromptTemplate: ì„ì‹œ ì½”ë“œ ì„¤ëª…] â†’ [LLM]
   â””â”€ "ignored"    â†’ ì¶œë ¥ ìƒëµ ë˜ëŠ” ê³ ì • ì‘ë‹µ
```




